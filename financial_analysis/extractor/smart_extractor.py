"""
æ™ºèƒ½æå–å™¨ - ç­–ç•¥æ¨¡å¼é‡æ„ç‰ˆ
Smart Extractor - Strategy Pattern Version

ä½œè€…: Lin Cifeng
åˆ›å»º: 2025-08-11
"""

import warnings
from pathlib import Path
from typing import Optional, Dict, Any, List
import pdfplumber
from datetime import datetime
import csv
import time
import concurrent.futures
import json
import hashlib
from tqdm import tqdm

warnings.filterwarnings('ignore')

from .base_extractor import BaseExtractor
from .financial_models import FinancialData
from .strategies import (
    RegexStrategy,
    LLMStrategy,
    OCRStrategy,
    TableStrategy,
    ExtractionResult
)


class SmartExtractor(BaseExtractor):
    """
    æ™ºèƒ½æå–å™¨ - ç­–ç•¥è°ƒåº¦å™¨
    
    æ”¯æŒçš„æå–æ¨¡å¼ï¼š
    - regex_only: ä»…ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
    - llm_only: ä»…ä½¿ç”¨LLM
    - regex_first: ä¼˜å…ˆæ­£åˆ™ï¼Œå…¶ä»–ç­–ç•¥è¡¥å……ï¼ˆé»˜è®¤ï¼‰
    - llm_first: ä¼˜å…ˆLLMï¼Œå…¶ä»–ç­–ç•¥è¡¥å……
    - adaptive: è‡ªé€‚åº”é€‰æ‹©æœ€ä½³ç­–ç•¥ç»„åˆ
    """
    
    def __init__(self, extraction_mode: str = 'regex_first', use_llm: bool = False):
        """
        åˆå§‹åŒ–æ™ºèƒ½æå–å™¨
        
        Args:
            extraction_mode: æå–æ¨¡å¼
            use_llm: æ˜¯å¦å¯ç”¨LLM
        """
        super().__init__()
        
        # éªŒè¯æå–æ¨¡å¼
        valid_modes = ['regex_only', 'regex_table', 'llm_only', 'regex_first', 'llm_first', 'adaptive']
        if extraction_mode not in valid_modes:
            print(f"  âš ï¸ æœªçŸ¥æ¨¡å¼ '{extraction_mode}'ï¼Œä½¿ç”¨é»˜è®¤ 'regex_first'")
            extraction_mode = 'regex_first'
        
        self.extraction_mode = extraction_mode
        self.use_llm = use_llm
        
        # åˆå§‹åŒ–ç­–ç•¥
        self.strategies = {
            'regex': RegexStrategy(),
            'table': TableStrategy(),
            'ocr': OCRStrategy()
        }
        
        # æ ¹æ®æ¨¡å¼å’Œè®¾ç½®åˆå§‹åŒ–LLMç­–ç•¥
        if extraction_mode in ['llm_only', 'llm_first'] or use_llm:
            self.strategies['llm'] = LLMStrategy()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_processed': 0,
            'complete_success': 0,
            'partial_success': 0,
            'failed': 0,
            'strategy_usage': {name: 0 for name in self.strategies.keys()}
        }
    
    def _extract_data(self, pdf: pdfplumber.PDF, result: FinancialData) -> None:
        """
        å®ç°ç­–ç•¥è°ƒåº¦é€»è¾‘
        
        Args:
            pdf: PDFå¯¹è±¡
            result: ç»“æœå¯¹è±¡
        """
        self.stats['total_processed'] += 1
        pdf_path = getattr(pdf, 'path', result.file_path)
        
        # æ ¹æ®æ¨¡å¼æ‰§è¡Œä¸åŒçš„ç­–ç•¥ç»„åˆ
        if self.extraction_mode == 'regex_only':
            extracted = self._extract_regex_only(pdf)
        elif self.extraction_mode == 'regex_table':
            extracted = self._extract_regex_table(pdf)
        elif self.extraction_mode == 'llm_only':
            extracted = self._extract_llm_only(pdf)
        elif self.extraction_mode == 'regex_first':
            extracted = self._extract_regex_first(pdf)
        elif self.extraction_mode == 'llm_first':
            extracted = self._extract_llm_first(pdf)
        else:  # adaptive
            extracted = self._extract_adaptive(pdf, pdf_path)
        
        # å¡«å……ç»“æœ
        self._fill_result(result, extracted)
        
        # æ›´æ–°ç»Ÿè®¡
        self._update_stats(result)
    
    def _extract_regex_only(self, pdf: pdfplumber.PDF) -> ExtractionResult:
        """ä»…ä½¿ç”¨æ­£åˆ™æå–ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰"""
        # æå–æ–‡æœ¬ - å¢åŠ æ‰«æé¡µæ•°ä»¥æé«˜æˆåŠŸç‡
        text = self.extract_text_from_pages(pdf, max_pages=30)
        unit_multiplier = self.detect_unit(text)
        
        # æ­£åˆ™æå–
        regex_result = self.strategies['regex'].execute(text, unit_multiplier=unit_multiplier)
        self.stats['strategy_usage']['regex'] += 1
        
        # æ¿€è¿›æ¨¡å¼ï¼šå¦‚æœæ­£åˆ™æ²¡æœ‰æå–åˆ°è¶³å¤Ÿæ•°æ®ï¼Œå°è¯•æå–ä»»ä½•å¤§æ•°å­—
        if regex_result.fields_count < 2:
            import re
            # æŸ¥æ‰¾æ‰€æœ‰å¤§æ•°å­—ï¼ˆç™¾ä¸‡çº§ä»¥ä¸Šï¼‰
            number_patterns = [
                r'\$\s*([0-9]{1,3}(?:,[0-9]{3})*(?:\.[0-9]+)?)\s*(?:million|Million|M|m)',
                r'([0-9]{1,3}(?:,[0-9]{3})*(?:\.[0-9]+)?)\s*(?:million|Million|åƒä¸‡|ç™¾ä¸‡|å„„)',
                r'\$\s*([0-9]{7,}(?:\.[0-9]+)?)',  # 7ä½æ•°ä»¥ä¸Š
                r'([0-9]{7,}(?:\.[0-9]+)?)\s*(?:å…ƒ|å††|â‚©|â‚¬)',
            ]
            
            all_numbers = []
            for pattern in number_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    try:
                        # æ¸…ç†æ•°å­—å¹¶è½¬æ¢
                        clean_num = match.replace(',', '')
                        value = float(clean_num)
                        if value > 1000000:  # åªä¿ç•™ç™¾ä¸‡ä»¥ä¸Šçš„æ•°å­—
                            all_numbers.append(value)
                    except:
                        pass
            
            # å°†æ‰¾åˆ°çš„æ•°å­—åˆ†é…ç»™ç¼ºå¤±çš„å­—æ®µ
            if all_numbers:
                all_numbers.sort(reverse=True)  # ä»å¤§åˆ°å°æ’åº
                if regex_result.total_assets is None and len(all_numbers) > 0:
                    regex_result.total_assets = all_numbers[0]
                if regex_result.total_liabilities is None and len(all_numbers) > 1:
                    regex_result.total_liabilities = all_numbers[1]
                if regex_result.revenue is None and len(all_numbers) > 2:
                    regex_result.revenue = all_numbers[2]
                if regex_result.net_profit is None and len(all_numbers) > 3:
                    regex_result.net_profit = all_numbers[3]
        
        # æ³¨æ„ï¼šregex_onlyæ¨¡å¼ä¸è¿›è¡Œè¡¨æ ¼æå–ï¼Œä»¥ä¿è¯é€Ÿåº¦
        regex_result.method = "regex"
        
        return regex_result
    
    def _extract_regex_table(self, pdf: pdfplumber.PDF) -> ExtractionResult:
        """ä½¿ç”¨æ­£åˆ™å’Œè¡¨æ ¼æå–ï¼ˆæ ‡å‡†æ¨¡å¼ï¼‰"""
        # æå–æ–‡æœ¬
        text = self.extract_text_from_pages(pdf, max_pages=50)
        unit_multiplier = self.detect_unit(text)
        
        # æ­£åˆ™æå–
        regex_result = self.strategies['regex'].execute(text, unit_multiplier=unit_multiplier)
        self.stats['strategy_usage']['regex'] += 1
        
        # è¡¨æ ¼æå–è¡¥å……
        table_result = self.strategies['table'].execute(pdf)
        self.stats['strategy_usage']['table'] += 1
        
        # åˆå¹¶ç»“æœ
        regex_result.merge(table_result)
        regex_result.method = "regex+table"
        
        return regex_result
    
    def _extract_llm_only(self, pdf: pdfplumber.PDF) -> ExtractionResult:
        """ä»…ä½¿ç”¨LLMæå– - æ”¹è¿›ç‰ˆï¼Œä¸“æ³¨è´¢åŠ¡æŠ¥è¡¨é¡µé¢"""
        if 'llm' not in self.strategies:
            print("  âŒ LLMç­–ç•¥ä¸å¯ç”¨")
            return ExtractionResult(method="failed")
        
        print("    ğŸ“– æ‰«æPDFå¯»æ‰¾è´¢åŠ¡æŠ¥è¡¨...")
        
        # æ›´ç²¾ç¡®çš„è´¢åŠ¡æŠ¥è¡¨å…³é”®è¯
        financial_keywords = [
            # è‹±æ–‡
            'total assets', 'total liabilities', 'total equity',
            'statement of financial position', 'balance sheet',
            'income statement', 'profit or loss',
            'comprehensive income', 'cash flow',
            # ä¸­æ–‡
            'æ€»èµ„äº§', 'æ€»è´Ÿå€º', 'å‡€èµ„äº§', 'èµ„äº§è´Ÿå€ºè¡¨',
            'åˆ©æ¶¦è¡¨', 'æŸç›Šè¡¨', 'ç°é‡‘æµé‡è¡¨',
            # è‘¡è„ç‰™è¯­ï¼ˆå·´è¥¿ï¼‰
            'ativo total', 'passivo total', 'patrimÃ´nio lÃ­quido',
            'demonstraÃ§Ã£o', 'balanÃ§o patrimonial',
            # è¥¿ç­ç‰™è¯­
            'activos totales', 'pasivos totales'
        ]
        
        # æ‰«æé¡µé¢ï¼Œæ‰¾åˆ°æœ€å¯èƒ½åŒ…å«è´¢åŠ¡æŠ¥è¡¨çš„é¡µé¢
        financial_pages = []
        table_pages = []  # åŒ…å«è¡¨æ ¼çš„é¡µé¢
        
        for i, page in enumerate(pdf.pages):
            text = page.extract_text()
            tables = page.extract_tables()
            
            if text:
                text_lower = text.lower()
                # è®¡ç®—å…³é”®è¯å¯†åº¦
                keyword_count = sum(1 for keyword in financial_keywords if keyword in text_lower)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ•°å­—ï¼ˆè´¢åŠ¡æ•°æ®é€šå¸¸åŒ…å«å¤§é‡æ•°å­—ï¼‰
                import re
                numbers = re.findall(r'\b\d{1,3}(?:,\d{3})*(?:\.\d+)?\b', text)
                
                if keyword_count > 0 and len(numbers) > 5:  # è‡³å°‘æœ‰1ä¸ªå…³é”®è¯å’Œ5ä¸ªæ•°å­—
                    financial_pages.append((i, text, keyword_count, len(numbers)))
            
            # è®°å½•æœ‰è¡¨æ ¼çš„é¡µé¢
            if tables:
                for table in tables:
                    if table and len(table) > 3:  # è‡³å°‘4è¡Œçš„è¡¨æ ¼
                        table_text = str(table).lower()
                        if any(kw in table_text for kw in ['asset', 'liabil', 'revenue', 'income', 'ativo', 'passivo']):
                            table_pages.append((i, text if text else ''))
                            break
        
        print(f"    ğŸ“Š æ‰¾åˆ° {len(financial_pages)} ä¸ªè´¢åŠ¡é¡µé¢ï¼Œ{len(table_pages)} ä¸ªè¡¨æ ¼é¡µé¢")
        
        # ç»„åˆæœ€ç›¸å…³çš„é¡µé¢
        combined_text = ""
        
        if financial_pages:
            # æŒ‰å…³é”®è¯å¯†åº¦å’Œæ•°å­—æ•°é‡æ’åº
            financial_pages.sort(key=lambda x: (x[2], x[3]), reverse=True)
            
            # ä¼˜å…ˆä½¿ç”¨å…³é”®è¯å¯†åº¦é«˜çš„é¡µé¢
            for page_num, text, keyword_count, number_count in financial_pages[:5]:
                print(f"      ğŸ“„ é¡µé¢ {page_num+1}: {keyword_count} ä¸ªå…³é”®è¯, {number_count} ä¸ªæ•°å­—")
                combined_text += f"\n\n--- Page {page_num+1} ---\n{text}"
                
                if len(combined_text) > 25000:  # é™åˆ¶æ€»é•¿åº¦
                    break
        
        # å¦‚æœè´¢åŠ¡é¡µé¢ä¸å¤Ÿï¼Œè¡¥å……è¡¨æ ¼é¡µé¢
        if len(combined_text) < 10000 and table_pages:
            for page_num, text in table_pages[:3]:
                if text and text not in combined_text:
                    combined_text += f"\n\n--- Table Page {page_num+1} ---\n{text}"
                    if len(combined_text) > 25000:
                        break
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨å‰20é¡µ
        if len(combined_text) < 1000:
            print("    âš ï¸ æœªæ‰¾åˆ°æ˜ç¡®çš„è´¢åŠ¡æŠ¥è¡¨ï¼Œä½¿ç”¨å‰20é¡µ")
            combined_text = self.extract_text_from_pages(pdf, max_pages=20)
        
        print(f"    ğŸ“ å‡†å¤‡å‘é€ {len(combined_text)} å­—ç¬¦ç»™LLM...")
        
        # è·å–å…¬å¸åå’Œå¹´ä»½ï¼ˆä»æ–‡ä»¶åæ¨æµ‹ï¼‰
        pdf_path = getattr(pdf, 'path', '')
        if pdf_path:
            from pathlib import Path
            filename = Path(pdf_path).stem
            parts = filename.split('_')
            company_name = parts[0] if parts else ''
            year = parts[1] if len(parts) > 1 else ''
        else:
            company_name = ''
            year = ''
        
        # LLMæå–
        llm_result = self.strategies['llm'].execute(
            combined_text, 
            text_limit=30000,
            company_name=company_name,
            year=year
        )
        self.stats['strategy_usage']['llm'] += 1
        
        return llm_result
    
    def _extract_regex_first(self, pdf: pdfplumber.PDF) -> ExtractionResult:
        """ä¼˜å…ˆæ­£åˆ™ï¼ŒLLMè¡¥å……"""
        # å…ˆæ‰§è¡Œæ­£åˆ™æå–
        result = self._extract_regex_only(pdf)
        
        # å¦‚æœä¸å®Œæ•´ä¸”æœ‰LLMï¼Œä½¿ç”¨LLMè¡¥å……
        if not result.is_complete and 'llm' in self.strategies:
            print(f"    ğŸ¤– ä½¿ç”¨LLMå¢å¼ºæå–ï¼ˆå½“å‰{result.fields_count}/4å­—æ®µï¼‰...")
            text = self.extract_text_from_pages(pdf, max_pages=50)
            llm_result = self.strategies['llm'].execute(text)
            self.stats['strategy_usage']['llm'] += 1
            
            result.merge(llm_result)
            result.method = "regex+table+llm"
        
        return result
    
    def _extract_llm_first(self, pdf: pdfplumber.PDF) -> ExtractionResult:
        """ä¼˜å…ˆLLMï¼Œæ­£åˆ™è¡¥å……"""
        # å…ˆæ‰§è¡ŒLLMæå–
        result = self._extract_llm_only(pdf)
        
        # å¦‚æœä¸å®Œæ•´ï¼Œä½¿ç”¨æ­£åˆ™è¡¥å……
        if not result.is_complete:
            regex_result = self._extract_regex_only(pdf)
            result.merge(regex_result)
            result.method = "llm+regex+table"
        
        return result
    
    def _extract_adaptive(self, pdf: pdfplumber.PDF, pdf_path: str) -> ExtractionResult:
        """è‡ªé€‚åº”ç­–ç•¥é€‰æ‹©"""
        # Step 1: æ£€æŸ¥æ˜¯å¦ä¸ºæ‰«æç‰ˆ
        method_prefix = ""
        if self.strategies['ocr'].can_handle(pdf):
            # æ‰§è¡ŒOCR
            ocr_result = self.strategies['ocr'].execute(pdf_path)
            self.stats['strategy_usage']['ocr'] += 1
            
            if hasattr(ocr_result, 'ocr_text'):
                text = ocr_result.ocr_text
                method_prefix = "ocr+"
            else:
                text = self.extract_text_from_pages(pdf, max_pages=50)
        else:
            text = self.extract_text_from_pages(pdf, max_pages=50)
        
        # Step 2: æ£€æµ‹å•ä½
        unit_multiplier = self.detect_unit(text)
        
        # Step 3: æ‰§è¡Œæ­£åˆ™æå–
        regex_result = self.strategies['regex'].execute(text, unit_multiplier=unit_multiplier)
        self.stats['strategy_usage']['regex'] += 1
        
        # Step 4: è¡¨æ ¼æå–è¡¥å……
        table_result = self.strategies['table'].execute(pdf)
        self.stats['strategy_usage']['table'] += 1
        regex_result.merge(table_result)
        
        # Step 5: å¦‚æœä¸å®Œæ•´ä¸”æœ‰LLMï¼Œä½¿ç”¨LLMè¡¥å……
        if not regex_result.is_complete and 'llm' in self.strategies:
            print(f"    ğŸ¤– ä½¿ç”¨LLMå¢å¼ºæå–ï¼ˆå½“å‰{regex_result.fields_count}/4å­—æ®µï¼‰...")
            llm_result = self.strategies['llm'].execute(text)
            self.stats['strategy_usage']['llm'] += 1
            regex_result.merge(llm_result)
            regex_result.method = f"{method_prefix}regex+table+llm"
        else:
            regex_result.method = f"{method_prefix}regex+table"
        
        return regex_result
    
    def _fill_result(self, result: FinancialData, extracted: ExtractionResult):
        """å¡«å……æå–ç»“æœåˆ°FinancialDataå¯¹è±¡"""
        result.total_assets = extracted.total_assets
        result.total_liabilities = extracted.total_liabilities
        result.revenue = extracted.revenue
        result.net_profit = extracted.net_profit
        result.confidence = extracted.confidence
        result.extraction_method = extracted.method
        
        # è®¾ç½®æˆåŠŸçº§åˆ«
        if extracted.is_complete:
            result.success_level = "Complete"
        elif extracted.fields_count > 0:
            result.success_level = f"Partial({extracted.fields_count}/4)"
        else:
            result.success_level = "Failed"
    
    def _update_stats(self, result: FinancialData):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        if result.success_level == "Complete":
            self.stats['complete_success'] += 1
        elif result.success_level and "Partial" in result.success_level:
            self.stats['partial_success'] += 1
        else:
            self.stats['failed'] += 1
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*60)
        print("æ™ºèƒ½æå–å™¨ç»Ÿè®¡")
        print("="*60)
        print(f"æå–æ¨¡å¼: {self.extraction_mode}")
        print(f"æ€»å¤„ç†æ–‡ä»¶: {self.stats['total_processed']}")
        
        if self.stats['total_processed'] > 0:
            complete_rate = self.stats['complete_success'] / self.stats['total_processed'] * 100
            partial_rate = self.stats['partial_success'] / self.stats['total_processed'] * 100
            failed_rate = self.stats['failed'] / self.stats['total_processed'] * 100
            
            print(f"\næˆåŠŸç‡ç»Ÿè®¡:")
            print(f"  å®Œå…¨æˆåŠŸ: {complete_rate:.1f}% ({self.stats['complete_success']}/{self.stats['total_processed']})")
            print(f"  éƒ¨åˆ†æˆåŠŸ: {partial_rate:.1f}% ({self.stats['partial_success']}/{self.stats['total_processed']})")
            print(f"  å¤±è´¥: {failed_rate:.1f}% ({self.stats['failed']}/{self.stats['total_processed']})")
            
            print(f"\nç­–ç•¥ä½¿ç”¨ç»Ÿè®¡:")
            for strategy, count in self.stats['strategy_usage'].items():
                if count > 0:
                    print(f"  {strategy}: {count}æ¬¡")


def smart_extract(
    input_dir: str = "data/raw_reports",
    output_dir: str = "output",
    limit: Optional[int] = None,
    extraction_mode: str = 'regex_first',
    use_llm: bool = False,
    max_workers: int = 1,
    use_cache: bool = False,
    batch_id: Optional[int] = None,
    batch_size: int = 200,
    skip_processed: bool = True,
    master_table_path: Optional[str] = None
) -> Dict[str, Any]:
    """
    æ™ºèƒ½æå–ä¸»å‡½æ•°
    
    Args:
        input_dir: PDFæ–‡ä»¶ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        limit: é™åˆ¶å¤„ç†æ–‡ä»¶æ•°
        extraction_mode: æå–æ¨¡å¼
        use_llm: æ˜¯å¦å¯ç”¨LLM
    """
    # è®°å½•å¼€å§‹æ—¶é—´
    total_start_time = time.time()
    
    # ç¼“å­˜å’Œä¸»è¡¨è®¾ç½®
    cache_dir = Path("output/extraction_cache")
    cache_dir.mkdir(parents=True, exist_ok=True)
    cache_file = cache_dir / "processed_files.json"
    
    # ä¸»æ§åˆ¶è¡¨
    if master_table_path:
        master_table_file = Path(master_table_path)
    else:
        master_table_file = Path("output/extraction_master.json")
    
    # åŠ è½½æˆ–åˆ›å»ºä¸»æ§åˆ¶è¡¨
    if master_table_file.exists():
        with open(master_table_file, 'r', encoding='utf-8') as f:
            master_table = json.load(f)
    else:
        master_table = {
            "metadata": {
                "total_files": 0,
                "processed": 0,
                "successful": 0,
                "partial": 0,
                "failed": 0,
                "last_update": datetime.now().isoformat(),
                "batches": {}
            },
            "files": {}
        }
    
    # åŠ è½½ç¼“å­˜
    processed_cache = {}
    if use_cache and cache_file.exists():
        with open(cache_file, 'r') as f:
            processed_cache = json.load(f)
    
    # è·å–PDFæ–‡ä»¶
    all_pdf_files = sorted(list(Path(input_dir).glob("*.pdf")))  # æ’åºä»¥ä¿è¯ä¸€è‡´æ€§
    master_table["metadata"]["total_files"] = len(all_pdf_files)
    
    # æ‰¹æ¬¡å¤„ç†
    if batch_id is not None:
        # è®¡ç®—æ‰¹æ¬¡èŒƒå›´
        start_idx = (batch_id - 1) * batch_size
        end_idx = min(start_idx + batch_size, len(all_pdf_files))
        pdf_files = all_pdf_files[start_idx:end_idx]
        print(f"å¤„ç†æ‰¹æ¬¡ {batch_id}: æ–‡ä»¶ {start_idx+1}-{end_idx} (å…±{len(pdf_files)}ä¸ª)")
        
        # è®°å½•æ‰¹æ¬¡ä¿¡æ¯
        master_table["metadata"]["batches"][str(batch_id)] = {
            "start": start_idx,
            "end": end_idx,
            "size": len(pdf_files),
            "status": "processing",
            "start_time": datetime.now().isoformat()
        }
    else:
        pdf_files = all_pdf_files
    
    # è¿‡æ»¤å·²å¤„ç†æ–‡ä»¶
    if skip_processed:
        original_count = len(pdf_files)
        # ç®€åŒ–é€»è¾‘ï¼šåªå¤„ç†æœªæˆåŠŸçš„æ–‡ä»¶
        filtered_files = []
        for f in pdf_files:
            # æ£€æŸ¥ä¸»è¡¨ä¸­çš„çŠ¶æ€
            if f.name in master_table["files"]:
                status = master_table["files"][f.name].get("status")
                # åªé‡æ–°å¤„ç†å¤±è´¥çš„æ–‡ä»¶ï¼Œè·³è¿‡æˆåŠŸå’Œéƒ¨åˆ†æˆåŠŸçš„
                if status in ["completed", "partial"]:
                    continue
            # å¦‚æœä¸åœ¨ä¸»è¡¨ä¸­ï¼Œæˆ–è€…çŠ¶æ€æ˜¯å¤±è´¥ï¼Œåˆ™éœ€è¦å¤„ç†
            filtered_files.append(f)
        
        pdf_files = filtered_files
        skipped_count = original_count - len(pdf_files)
        if skipped_count > 0:
            print(f"è·³è¿‡ {skipped_count} ä¸ªå·²å¤„ç†æ–‡ä»¶")
    
    if limit:
        pdf_files = pdf_files[:limit]
    
    print(f"\n{'='*60}")
    print(f"æ™ºèƒ½è´¢åŠ¡æ•°æ®æå– - æ‰¹é‡ç®¡ç†æ¨¡å¼")
    print(f"{'='*60}")
    if batch_id:
        print(f"æ‰¹æ¬¡ID: {batch_id}")
    print(f"æ€»æ–‡ä»¶æ•°: {master_table['metadata']['total_files']}")
    print(f"å·²å¤„ç†: {master_table['metadata']['processed']}")
    print(f"æœ¬æ¬¡å¾…å¤„ç†: {len(pdf_files)}")
    print(f"æå–æ¨¡å¼: {extraction_mode}")
    print(f"LLMæ”¯æŒ: {'å¯ç”¨' if use_llm else 'çŒç”¨'}")
    print(f"å¹¶è¡Œçº¿ç¨‹: {max_workers}")
    print(f"ç¼“å­˜: {'å¯ç”¨' if use_cache else 'ç¦ç”¨'}")
    print(f"{'='*60}")
    
    # å®šä¹‰å•æ–‡ä»¶å¤„ç†å‡½æ•°
    def process_single_file(pdf_path: Path) -> FinancialData:
        try:
            # æ£€æŸ¥é‡è¯•æ¬¡æ•°
            retry_count = master_table["files"].get(pdf_path.name, {}).get("retry_count", 0)
            
            # å¦‚æœå·²ç»é‡è¯•å¤šæ¬¡ï¼Œç›´æ¥è·³è¿‡
            if retry_count > 3:
                print(f"  â­ï¸ è·³è¿‡å¤šæ¬¡å¤±è´¥æ–‡ä»¶: {pdf_path.name} (å·²é‡è¯•{retry_count}æ¬¡)")
                result = FinancialData(company=pdf_path.stem.split('_')[0], file_path=str(pdf_path))
                result.success_level = "Skipped"
                return result
            
            # æ£€æŸ¥ç¼“å­˜
            if use_cache and pdf_path.name in processed_cache:
                cached = processed_cache[pdf_path.name]
                # åªæœ‰æˆåŠŸæˆ–éƒ¨åˆ†æˆåŠŸçš„æ‰ä½¿ç”¨ç¼“å­˜
                if cached.get('success_level') not in ['Failed', 'Skipped']:
                    result = FinancialData(company=pdf_path.stem.split('_')[0], file_path=str(pdf_path))
                    result.company = cached.get('company')
                    result.year = cached.get('year')
                    result.total_assets = cached.get('total_assets')
                    result.total_liabilities = cached.get('total_liabilities')
                    result.revenue = cached.get('revenue')
                    result.net_profit = cached.get('net_profit')
                    result.success_level = cached.get('success_level')
                    result.extraction_method = "cached"
                    return result
            
            # åˆ›å»ºæ–°çš„æå–å™¨å®ä¾‹ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
            # å¯¹äºé‡è¯•çš„æ–‡ä»¶ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„ç­–ç•¥
            if retry_count > 0:
                # é‡è¯•æ—¶å°è¯•ä½¿ç”¨ä¸åŒçš„æ¨¡å¼
                retry_mode = 'regex_only' if extraction_mode == 'llm_only' else 'regex_first'
                extractor = SmartExtractor(extraction_mode=retry_mode, use_llm=False)
            else:
                extractor = SmartExtractor(extraction_mode=extraction_mode, use_llm=use_llm)
            
            result = extractor.extract_from_pdf(str(pdf_path))
            
            # ä¿å­˜åˆ°ç¼“å­˜å’Œä¸»è¡¨
            file_info = {
                'company': result.company,
                'year': result.year,
                'total_assets': result.total_assets,
                'total_liabilities': result.total_liabilities,
                'revenue': result.revenue,
                'net_profit': result.net_profit,
                'success_level': result.success_level,
                'timestamp': datetime.now().isoformat()
            }
            
            if use_cache and result.success_level != "Failed":
                processed_cache[pdf_path.name] = file_info
            
            # æ›´æ–°ä¸»è¡¨
            extracted_fields = sum([
                1 for field in [result.total_assets, result.total_liabilities, 
                               result.revenue, result.net_profit]
                if field is not None
            ])
            
            master_table["files"][pdf_path.name] = {
                "status": "completed" if result.success_level == "Complete" else 
                         "partial" if "Partial" in str(result.success_level) else "failed",
                "batch_id": batch_id,
                "extracted_fields": extracted_fields,
                "quality_score": extracted_fields / 4.0,
                "retry_count": master_table["files"].get(pdf_path.name, {}).get("retry_count", 0),
                "last_update": datetime.now().isoformat()
            }
            
            return result
        except Exception as e:
            print(f"  âŒ {pdf_path.name}: {e}")
            result = FinancialData(company=pdf_path.stem.split('_')[0], file_path=str(pdf_path))
            result.success_level = "Failed"
            return result
    
    # æå–æ•°æ®
    results = []
    
    if max_workers > 1:
        # å¹¶è¡Œå¤„ç†
        # LLMæ¨¡å¼é™åˆ¶å¹¶å‘æ•°ï¼ˆAPIé™åˆ¶ï¼‰
        if extraction_mode == 'llm_only' and use_llm:
            actual_workers = min(max_workers, 2)
        else:
            actual_workers = max_workers
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=actual_workers) as executor:
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            with tqdm(total=len(pdf_files), desc="å¤„ç†è¿›åº¦") as pbar:
                future_to_pdf = {executor.submit(process_single_file, pdf): pdf for pdf in pdf_files}
                
                for future in concurrent.futures.as_completed(future_to_pdf):
                    pdf = future_to_pdf[future]
                    try:
                        # å‡å°‘è¶…æ—¶æ—¶é—´åˆ°30ç§’ï¼Œå¿«é€Ÿè·³è¿‡é—®é¢˜æ–‡ä»¶
                        result = future.result(timeout=30)
                        results.append(result)
                        
                        # æ›´æ–°è¿›åº¦æ¡
                        if result.success_level == "Complete":
                            status = "âœ…"
                        elif "Partial" in str(result.success_level):
                            status = "âš ï¸"
                        else:
                            status = "âŒ"
                        pbar.set_description(f"{pdf.name[:30]} {status}")
                        pbar.update(1)
                        
                        # æ¯å¤„ç†10ä¸ªæ–‡ä»¶ä¿å­˜ä¸€æ¬¡è¿›åº¦
                        if len(results) % 10 == 0:
                            # ä¿å­˜ä¸­é—´è¿›åº¦
                            if use_cache:
                                with open(cache_file, 'w') as f:
                                    json.dump(processed_cache, f, indent=2)
                            with open(master_table_file, 'w', encoding='utf-8') as f:
                                json.dump(master_table, f, indent=2, ensure_ascii=False)
                            
                    except concurrent.futures.TimeoutError:
                        print(f"\n  â±ï¸ {pdf.name}: å¤„ç†è¶…æ—¶(60ç§’)ï¼Œæ ‡è®°ä¸ºå¤±è´¥")
                        # åˆ›å»ºå¤±è´¥ç»“æœ
                        failed_result = FinancialData(company=pdf.stem.split('_')[0], file_path=str(pdf))
                        failed_result.success_level = "Failed"
                        results.append(failed_result)
                        # æ ‡è®°éœ€è¦é‡è¯•
                        master_table["files"][pdf.name] = {
                            "status": "failed",
                            "retry_needed": True,
                            "error": "Timeout",
                            "timestamp": datetime.now().isoformat()
                        }
                        pbar.update(1)
                    except Exception as e:
                        print(f"\n  âŒ {pdf.name}: {str(e)[:100]}")
                        # åˆ›å»ºå¤±è´¥ç»“æœä½†ç»§ç»­å¤„ç†
                        failed_result = FinancialData(company=pdf.stem.split('_')[0], file_path=str(pdf))
                        failed_result.success_level = "Failed"
                        results.append(failed_result)
                        pbar.update(1)
    else:
        # ä¸²è¡Œå¤„ç†ï¼ˆåŸé€»è¾‘ï¼‰
        for i, pdf_path in enumerate(pdf_files, 1):
            print(f"\n[{i}/{len(pdf_files)}] {pdf_path.name}")
            result = process_single_file(pdf_path)
            results.append(result)
            
            # æ‰“å°ç»“æœæ‘˜è¦
            if result.success_level == "Complete":
                print(f"  âœ… å®Œå…¨æˆåŠŸ - {result.extraction_method}")
            elif "Partial" in str(result.success_level):
                print(f"  âš ï¸ éƒ¨åˆ†æˆåŠŸ - {result.success_level} - {result.extraction_method}")
            else:
                print(f"  âŒ å¤±è´¥")
    
    # ä¿å­˜ç¼“å­˜å’Œä¸»è¡¨
    if use_cache:
        with open(cache_file, 'w') as f:
            json.dump(processed_cache, f, indent=2)
    
    # æ›´æ–°ä¸»è¡¨å…ƒæ•°æ®
    master_table["metadata"]["processed"] = len([f for f in master_table["files"].values() 
                                                 if f.get("status") in ["completed", "partial"]])
    master_table["metadata"]["successful"] = len([f for f in master_table["files"].values() 
                                                   if f.get("status") == "completed"])
    master_table["metadata"]["partial"] = len([f for f in master_table["files"].values() 
                                               if f.get("status") == "partial"])
    master_table["metadata"]["failed"] = len([f for f in master_table["files"].values() 
                                              if f.get("status") == "failed"])
    master_table["metadata"]["last_update"] = datetime.now().isoformat()
    
    # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
    if batch_id and str(batch_id) in master_table["metadata"]["batches"]:
        master_table["metadata"]["batches"][str(batch_id)]["status"] = "completed"
        master_table["metadata"]["batches"][str(batch_id)]["end_time"] = datetime.now().isoformat()
    
    # ä¿å­˜ä¸»è¡¨
    with open(master_table_file, 'w', encoding='utf-8') as f:
        json.dump(master_table, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜ç»“æœ
    output_path = Path(output_dir)
    results_dir = output_path / "results"
    results_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d%H%M')
    prefix = "parallel_" if max_workers > 1 else ""
    output_file = results_dir / f"{prefix}extraction_{timestamp}.csv"
    
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['Company', 'Year', 'Total Assets', 'Total Liabilities',
                     'Revenue', 'Net Profit', 'Method', 'File', 'Status',
                     'Success Level', 'Currency', 'Unit', 'Language']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for result in results:
            row = result.to_dict()
            row['Method'] = result.extraction_method
            writer.writerow(row)
    
    # æ‰“å°ç»Ÿè®¡ï¼ˆåªåœ¨å•çº¿ç¨‹æ¨¡å¼ä¸‹æœ‰extractorå®ä¾‹ï¼‰
    if max_workers == 1:
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶extractorå®ä¾‹æ¥æ‰“å°ç»Ÿè®¡
        temp_extractor = SmartExtractor(extraction_mode=extraction_mode, use_llm=use_llm)
        temp_extractor.stats['total_processed'] = len(results)
        temp_extractor.stats['complete_success'] = sum(1 for r in results if r.success_level == "Complete")
        temp_extractor.stats['partial_success'] = sum(1 for r in results if "Partial" in str(r.success_level))
        temp_extractor.stats['failed'] = sum(1 for r in results if r.success_level == "Failed")
        temp_extractor.print_stats()
    else:
        # å¤šçº¿ç¨‹æ¨¡å¼ä¸‹ç›´æ¥æ‰“å°ç»Ÿè®¡
        print("\n" + "="*60)
        print("æå–ç»Ÿè®¡")
        print("="*60)
        complete = sum(1 for r in results if r.success_level == "Complete")
        partial = sum(1 for r in results if "Partial" in str(r.success_level))
        failed = sum(1 for r in results if r.success_level == "Failed")
        cached = sum(1 for r in results if hasattr(r, 'extraction_method') and r.extraction_method == "cached")
        
        total = len(results)
        if total > 0:
            print(f"æ€»æ–‡ä»¶æ•°: {total}")
            print(f"  å®Œå…¨æˆåŠŸ: {complete} ({complete/total*100:.1f}%)")
            print(f"  éƒ¨åˆ†æˆåŠŸ: {partial} ({partial/total*100:.1f}%)")
            print(f"  å¤±è´¥: {failed} ({failed/total*100:.1f}%)")
            if cached > 0:
                print(f"  ä½¿ç”¨ç¼“å­˜: {cached}")
    
    # æ˜¾ç¤ºæ€»æ‰§è¡Œæ—¶é—´
    total_elapsed = time.time() - total_start_time
    print(f"\næ€»æ‰§è¡Œæ—¶é—´: {total_elapsed:.2f}ç§’")
    if len(pdf_files) > 0:
        print(f"å¹³å‡æ¯æ–‡ä»¶: {total_elapsed/len(pdf_files):.2f}ç§’")
    
    print(f"\nç»“æœå·²ä¿å­˜è‡³: {output_file}")
    print(f"ä¸»æ§åˆ¶è¡¨å·²æ›´æ–°: {master_table_file}")
    
    # è¿”å›ç»Ÿè®¡ä¿¡æ¯
    return {
        "total_processed": len(results),
        "successful": complete if 'complete' in locals() else sum(1 for r in results if r.success_level == "Complete"),
        "partial": partial if 'partial' in locals() else sum(1 for r in results if "Partial" in str(r.success_level)),
        "failed": failed if 'failed' in locals() else sum(1 for r in results if r.success_level == "Failed"),
        "batch_id": batch_id,
        "elapsed_time": total_elapsed
    }


if __name__ == "__main__":
    # æµ‹è¯•ç­–ç•¥æ¨¡å¼
    smart_extract(limit=5, extraction_mode='regex_first')