"""
æ‰¹é‡æå–ç®¡ç†å™¨
Batch Extraction Manager

ä½œè€…: Lin Cifeng
åˆ›å»º: 2025-08-12
"""

import json
import time
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import warnings
warnings.filterwarnings('ignore')

from .smart_extractor import smart_extract


def load_master_table() -> Dict:
    """åŠ è½½ä¸»æ§åˆ¶è¡¨"""
    master_file = Path("output/extraction_master.json")
    if master_file.exists():
        with open(master_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        "metadata": {
            "total_files": 0,
            "processed": 0,
            "successful": 0,
            "partial": 0,
            "failed": 0,
            "last_update": None,
            "batches": {}
        },
        "files": {}
    }


def show_status():
    """æ˜¾ç¤ºæå–è¿›åº¦"""
    master = load_master_table()
    meta = master["metadata"]
    
    print("\n" + "="*80)
    print("è´¢æŠ¥æå–è¿›åº¦ç›‘æ§")
    print("="*80)
    
    # æ€»ä½“è¿›åº¦
    total = meta["total_files"]
    processed = meta["processed"]
    
    if total > 0:
        progress_pct = processed / total * 100
        progress_bar = "â–ˆ" * int(progress_pct / 5) + "â–‘" * (20 - int(progress_pct / 5))
        print(f"\næ€»è¿›åº¦: [{progress_bar}] {progress_pct:.1f}% ({processed}/{total})")
    else:
        print("\nå°šæœªå¼€å§‹æå–")
    
    # è´¨é‡ç»Ÿè®¡
    print(f"\næå–è´¨é‡:")
    print(f"  âœ… å®Œå…¨æˆåŠŸ: {meta['successful']} ä»½")
    print(f"  âš ï¸  éƒ¨åˆ†æˆåŠŸ: {meta['partial']} ä»½")
    print(f"  âŒ å¤±è´¥: {meta['failed']} ä»½")
    
    # æ‰¹æ¬¡è¿›åº¦
    if meta["batches"]:
        print(f"\næ‰¹æ¬¡è¿›åº¦:")
        for batch_id, batch_info in sorted(meta["batches"].items(), key=lambda x: int(x[0])):
            status_icon = "âœ…" if batch_info["status"] == "completed" else "â¸ï¸" if batch_info["status"] == "processing" else "â³"
            print(f"  æ‰¹æ¬¡ {batch_id}: {status_icon} {batch_info['status']} ({batch_info['size']} ä»½)")
    
    # æœ€è¿‘æ›´æ–°
    if meta["last_update"]:
        update_time = datetime.fromisoformat(meta["last_update"])
        print(f"\næœ€åæ›´æ–°: {update_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # å»ºè®®ä¸‹ä¸€æ­¥
    if processed < total:
        remaining = total - processed
        next_batch = (processed // 200) + 1
        print(f"\nğŸ’¡ å»ºè®®:")
        print(f"  è¿˜æœ‰ {remaining} ä»½å¾…å¤„ç†")
        print(f"  ä¸‹ä¸€æ‰¹æ¬¡: python main.py extract --batch {next_batch}")


def retry_failed(failed_only: bool = True, partial_only: bool = False, mode: str = "llm_only"):
    """é‡è¯•å¤±è´¥æˆ–éƒ¨åˆ†æˆåŠŸçš„æ–‡ä»¶"""
    master = load_master_table()
    
    # ç­›é€‰éœ€è¦é‡è¯•çš„æ–‡ä»¶
    retry_files = []
    for filename, info in master["files"].items():
        if failed_only and info["status"] == "failed":
            retry_files.append(filename)
        elif partial_only and info["status"] == "partial":
            retry_files.append(filename)
        elif not failed_only and not partial_only and info["status"] in ["failed", "partial"]:
            retry_files.append(filename)
    
    if not retry_files:
        print("æ²¡æœ‰éœ€è¦é‡è¯•çš„æ–‡ä»¶")
        return
    
    print(f"\nå‡†å¤‡é‡è¯• {len(retry_files)} ä¸ªæ–‡ä»¶")
    print(f"ä½¿ç”¨æ¨¡å¼: {mode}")
    
    # åˆ†æ‰¹é‡è¯•
    batch_size = 50
    for i in range(0, len(retry_files), batch_size):
        batch = retry_files[i:i+batch_size]
        print(f"\né‡è¯•æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} ä¸ªæ–‡ä»¶")
        
        # è°ƒç”¨smart_extracté‡è¯•
        stats = smart_extract(
            extraction_mode=mode,
            use_llm=True if "llm" in mode else False,
            max_workers=2 if "llm" in mode else 4,
            use_cache=True,
            limit=len(batch)
        )
        
        print(f"é‡è¯•ç»“æœ: æˆåŠŸ={stats['successful']}, éƒ¨åˆ†={stats['partial']}, å¤±è´¥={stats['failed']}")


def generate_quality_report():
    """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
    master = load_master_table()
    
    print("\n" + "="*80)
    print("è´¢æŠ¥æå–è´¨é‡æŠ¥å‘Š")
    print("="*80)
    
    # æŒ‰è´¨é‡åˆ†ç»„
    high_quality = []   # 4/4 å­—æ®µ
    medium_quality = [] # 2-3 å­—æ®µ
    low_quality = []    # 0-1 å­—æ®µ
    
    for filename, info in master["files"].items():
        fields = info.get("extracted_fields", 0)
        if fields == 4:
            high_quality.append(filename)
        elif fields >= 2:
            medium_quality.append(filename)
        else:
            low_quality.append(filename)
    
    # ç»Ÿè®¡
    total = len(master["files"])
    if total > 0:
        print(f"\nè´¨é‡åˆ†å¸ƒ:")
        print(f"  ğŸŒŸ é«˜è´¨é‡ (4/4å­—æ®µ): {len(high_quality)} ({len(high_quality)/total*100:.1f}%)")
        print(f"  â­ ä¸­è´¨é‡ (2-3å­—æ®µ): {len(medium_quality)} ({len(medium_quality)/total*100:.1f}%)")
        print(f"  âšª ä½è´¨é‡ (0-1å­—æ®µ): {len(low_quality)} ({len(low_quality)/total*100:.1f}%)")
    
    # æ‰¹æ¬¡åˆ†æ
    if master["metadata"]["batches"]:
        print(f"\næ‰¹æ¬¡åˆ†æ:")
        for batch_id, batch_info in sorted(master["metadata"]["batches"].items(), key=lambda x: int(x[0])):
            if batch_info["status"] == "completed":
                # ç»Ÿè®¡è¯¥æ‰¹æ¬¡çš„æˆåŠŸç‡
                batch_files = [f for f, info in master["files"].items() 
                             if info.get("batch_id") == int(batch_id)]
                if batch_files:
                    batch_success = sum(1 for f in batch_files 
                                      if master["files"][f]["status"] == "completed")
                    success_rate = batch_success / len(batch_files) * 100
                    print(f"  æ‰¹æ¬¡ {batch_id}: {success_rate:.1f}% å®Œå…¨æˆåŠŸç‡")
    
    # éœ€è¦äººå·¥å¤æ ¸çš„æ–‡ä»¶
    review_needed = [f for f in low_quality if master["files"][f]["status"] == "partial"]
    if review_needed:
        print(f"\nâš ï¸ éœ€è¦äººå·¥å¤æ ¸: {len(review_needed)} ä»½")
        print("  å»ºè®®ä½¿ç”¨LLMæ¨¡å¼é‡è¯•æˆ–æ‰‹åŠ¨æ£€æŸ¥")
        
        # ä¿å­˜éœ€è¦å¤æ ¸çš„åˆ—è¡¨
        review_file = Path("output/files_need_review.txt")
        with open(review_file, 'w', encoding='utf-8') as f:
            for filename in review_needed[:20]:  # æ˜¾ç¤ºå‰20ä¸ª
                f.write(f"{filename}\n")
        print(f"  è¯¦ç»†åˆ—è¡¨å·²ä¿å­˜è‡³: {review_file}")
    
    # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
    report_file = Path("output/extraction_quality_report.json")
    report_data = {
        "generated_at": datetime.now().isoformat(),
        "summary": {
            "total": total,
            "high_quality": len(high_quality),
            "medium_quality": len(medium_quality),
            "low_quality": len(low_quality)
        },
        "files": {
            "high_quality": high_quality[:100],  # ä¿å­˜å‰100ä¸ª
            "medium_quality": medium_quality[:100],
            "low_quality": low_quality[:100]
        }
    }
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")


def batch_extract_all(batch_size: int = 200, mode: str = "regex_only", max_workers: int = 4):
    """æ‰¹é‡æå–æ‰€æœ‰æ–‡ä»¶"""
    # è·å–æ€»æ–‡ä»¶æ•°
    pdf_dir = Path("data/raw_reports")
    all_pdfs = list(pdf_dir.glob("*.pdf"))
    total_files = len(all_pdfs)
    
    print(f"\nå‡†å¤‡æ‰¹é‡æå– {total_files} ä»½è´¢æŠ¥")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"æå–æ¨¡å¼: {mode}")
    print(f"å¹¶è¡Œçº¿ç¨‹: {max_workers}")
    
    # è®¡ç®—æ‰¹æ¬¡æ•°
    num_batches = (total_files + batch_size - 1) // batch_size
    
    print(f"å°†åˆ†ä¸º {num_batches} ä¸ªæ‰¹æ¬¡å¤„ç†")
    
    # é€æ‰¹å¤„ç†
    for batch_id in range(1, num_batches + 1):
        print(f"\n{'='*60}")
        print(f"å¤„ç†æ‰¹æ¬¡ {batch_id}/{num_batches}")
        print(f"{'='*60}")
        
        stats = smart_extract(
            batch_id=batch_id,
            batch_size=batch_size,
            extraction_mode=mode,
            use_llm="llm" in mode,
            max_workers=max_workers,
            use_cache=True,
            skip_processed=True
        )
        
        print(f"æ‰¹æ¬¡ {batch_id} å®Œæˆ: æˆåŠŸ={stats['successful']}, éƒ¨åˆ†={stats['partial']}, å¤±è´¥={stats['failed']}")
        
        # æ¯5ä¸ªæ‰¹æ¬¡æ˜¾ç¤ºä¸€æ¬¡æ€»è¿›åº¦
        if batch_id % 5 == 0:
            show_status()
    
    # æœ€ç»ˆæŠ¥å‘Š
    print("\n" + "="*80)
    print("æ‰¹é‡æå–å®Œæˆï¼")
    show_status()
    generate_quality_report()


def monitor_extraction():
    """å®æ—¶ç›‘æ§æå–è¿›åº¦"""
    print("\n" + "="*70)
    print("ğŸ“Š è´¢æŠ¥å…¨é‡æå–å®æ—¶ç›‘æ§")
    print("="*70)
    
    last_count = 0
    monitor_start = time.time()
    
    while True:
        try:
            # è¯»å–ä¸»æ§åˆ¶è¡¨
            master = load_master_table()
            
            # å®é™…ç»Ÿè®¡æ–‡ä»¶çŠ¶æ€
            total = len([f for f in Path('data/raw_reports').glob('*.pdf')])
            all_files = master.get('files', {})
            processed = len(all_files)
            successful = len([f for f, d in all_files.items() if d.get('status') == 'completed'])
            partial = len([f for f, d in all_files.items() if d.get('status') == 'partial'])
            failed = len([f for f, d in all_files.items() if d.get('status') == 'failed'])
            
            # è®¡ç®—é€Ÿåº¦
            elapsed = time.time() - monitor_start
            if processed > last_count and elapsed > 0:
                recent_speed = (processed - last_count) / 5  # æœ€è¿‘5ç§’çš„é€Ÿåº¦
                overall_speed = processed / (elapsed / 60)  # æ•´ä½“é€Ÿåº¦ï¼ˆæ–‡ä»¶/åˆ†é’Ÿï¼‰
                eta = (total - processed) / recent_speed if recent_speed > 0 else 0
                eta_min = int(eta / 60)
                eta_sec = int(eta % 60)
            else:
                recent_speed = 0
                overall_speed = 0
                eta_min = 0
                eta_sec = 0
            
            # è·å–å½“å‰æ‰¹æ¬¡ä¿¡æ¯
            batches = master.get('metadata', {}).get('batches', {})
            current_batch = None
            for bid, info in batches.items():
                if info.get('status') == 'processing':
                    current_batch = bid
                    break
            
            # æ¸…å±å¹¶æ˜¾ç¤º
            print("\033[2J\033[H")  # æ¸…å±
            print("="*70)
            print("ğŸ“Š è´¢æŠ¥å…¨é‡æå–å®æ—¶ç›‘æ§")
            print("="*70)
            print(f"æ€»æ–‡ä»¶æ•°: {total}")
            print(f"å·²å¤„ç†: {processed} ({processed/total*100:.1f}%)")
            print(f"  âœ… æˆåŠŸ: {successful} ({successful/total*100:.1f}%)")
            print(f"  âš ï¸ éƒ¨åˆ†: {partial} ({partial/total*100:.1f}%)")
            print(f"  âŒ å¤±è´¥: {failed} ({failed/total*100:.1f}%)")
            
            if current_batch:
                print(f"å½“å‰æ‰¹æ¬¡: {current_batch}")
            
            print("-"*70)
            print(f"ç¬æ—¶é€Ÿåº¦: {recent_speed*60:.1f} æ–‡ä»¶/åˆ†é’Ÿ")
            print(f"å¹³å‡é€Ÿåº¦: {overall_speed:.1f} æ–‡ä»¶/åˆ†é’Ÿ")
            print(f"é¢„è®¡å‰©ä½™: {eta_min}åˆ†{eta_sec}ç§’")
            print(f"è¿è¡Œæ—¶é—´: {int(elapsed/60)}åˆ†{int(elapsed%60)}ç§’")
            print("-"*70)
            
            # æ˜¾ç¤ºè¿›åº¦æ¡
            bar_length = 50
            filled = int(bar_length * processed / total) if total > 0 else 0
            bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
            print(f"è¿›åº¦: [{bar}] {processed}/{total}")
            
            # æ˜¾ç¤ºæœ€è¿‘å¤„ç†çš„æ–‡ä»¶
            recent_files = []
            for name, info in all_files.items():
                if 'last_update' in info:
                    recent_files.append((name, info['last_update'], info.get('status')))
            
            if recent_files:
                recent_files.sort(key=lambda x: x[1], reverse=True)
                print("\næœ€è¿‘å¤„ç†:")
                for name, _, status in recent_files[:3]:
                    emoji = 'âœ…' if status == 'completed' else 'âš ï¸' if status == 'partial' else 'âŒ'
                    print(f"  {emoji} {name[:50]}")
            
            last_count = processed
            time.sleep(5)
            
        except KeyboardInterrupt:
            print("\n\nç›‘æ§å·²åœæ­¢")
            break
        except Exception as e:
            print(f"é”™è¯¯: {e}")
            time.sleep(5)


def merge_all_results(output_prefix: str = 'output/final_combined_results') -> Optional[Dict]:
    """åˆå¹¶æ‰€æœ‰æå–ç»“æœ"""
    results_dir = Path('output/results')
    archive_dir = Path('output/archive')
    all_dfs = []
    
    # è¯»å–resultsç›®å½•
    for csv_file in results_dir.glob('*.csv'):
        try:
            df = pd.read_csv(csv_file)
            if len(df) > 0:
                all_dfs.append(df)
                print(f'è¯»å– {csv_file.name}: {len(df)} æ¡')
        except:
            pass
    
    # è¯»å–archiveç›®å½•
    if archive_dir.exists():
        for batch_dir in archive_dir.glob('batch_results_*'):
            for csv_file in batch_dir.glob('*.csv'):
                try:
                    df = pd.read_csv(csv_file)
                    if len(df) > 0:
                        all_dfs.append(df)
                        print(f'è¯»å–å½’æ¡£ {csv_file.name}: {len(df)} æ¡')
                except:
                    pass
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    if all_dfs:
        combined_df = pd.concat(all_dfs, ignore_index=True)
        
        # è§„èŒƒåŒ–åˆ—å
        column_mapping = {
            'company': 'Company',
            'year': 'Year', 
            'total_assets': 'Total Assets',
            'total_liabilities': 'Total Liabilities',
            'revenue': 'Revenue',
            'net_profit': 'Net Profit',
            'success_level': 'Success Level',
            'method': 'Method',
            'file': 'File'
        }
        
        # é‡å‘½ååˆ—
        for old_col, new_col in column_mapping.items():
            if old_col in combined_df.columns and new_col not in combined_df.columns:
                combined_df.rename(columns={old_col: new_col}, inplace=True)
        
        # å»é‡ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
        if 'File' in combined_df.columns:
            combined_df = combined_df.drop_duplicates(subset=['File'], keep='last')
        
        # æ¸…ç†æ•°æ® - åªä¿ç•™æœ‰æ•ˆæ•°æ®
        if 'Success Level' in combined_df.columns:
            combined_df = combined_df[combined_df['Success Level'].notna()]
            combined_df = combined_df[combined_df['Success Level'] != 'Failed']
        
        print(f'\nåˆå¹¶åæ€»è®°å½•æ•°: {len(combined_df)}')
        
        # ç»Ÿè®¡
        if 'Success Level' in combined_df.columns:
            print('\næˆåŠŸçº§åˆ«åˆ†å¸ƒ:')
            success_counts = combined_df['Success Level'].value_counts()
            for level, count in success_counts.items():
                print(f'  {level}: {count}')
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        output_file = Path(f'{output_prefix}.csv')
        combined_df.to_csv(output_file, index=False)
        
        # ä¿å­˜Excelç‰ˆæœ¬
        excel_file = Path(f'{output_prefix}.xlsx')
        combined_df.to_excel(excel_file, index=False)
        
        print(f'\nâœ… æœ€ç»ˆåˆå¹¶ç»“æœ:')
        print(f'  æ€»è®°å½•æ•°: {len(combined_df)}')
        
        # åˆ†ææ•°æ®è´¨é‡
        print(f'\nğŸ“Š æ•°æ®è´¨é‡åˆ†æ:')
        for col in ['Total Assets', 'Total Liabilities', 'Revenue', 'Net Profit']:
            if col in combined_df.columns:
                non_null = combined_df[col].notna().sum()
                pct = non_null / len(combined_df) * 100
                print(f'  {col}: {non_null}/{len(combined_df)} ({pct:.1f}%)')
        
        # å¹´ä»½åˆ†å¸ƒ
        if 'Year' in combined_df.columns:
            print(f'\nğŸ“… å¹´ä»½åˆ†å¸ƒ:')
            year_counts = combined_df['Year'].value_counts().head(5)
            for year, count in year_counts.items():
                print(f'  {year}: {count}')
        
        # å…¬å¸è¦†ç›–
        if 'Company' in combined_df.columns:
            print(f'\nğŸ¢ å…¬å¸è¦†ç›–: {combined_df["Company"].nunique()} å®¶')
        
        print(f'\nğŸ’¾ æ–‡ä»¶å·²ä¿å­˜:')
        print(f'  CSV: {output_file}')
        print(f'  Excel: {excel_file}')
        
        return {
            'total': len(combined_df),
            'companies': combined_df['Company'].nunique() if 'Company' in combined_df.columns else 0,
            'csv_file': str(output_file),
            'excel_file': str(excel_file)
        }
    else:
        print('æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶')
        return None


def generate_final_report():
    """ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š"""
    # å…ˆåˆå¹¶æ‰€æœ‰ç»“æœ
    merge_stats = merge_all_results()
    
    if merge_stats:
        # è¯»å–åˆå¹¶åçš„æ•°æ®
        df = pd.read_csv(merge_stats['csv_file'])
        
        # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        from ..visualization import create_charts
        create_charts()
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        report_path = Path('output/reports/final_report.txt')
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("è´¢åŠ¡æ•°æ®æå–æœ€ç»ˆæŠ¥å‘Š\n")
            f.write("="*80 + "\n\n")
            
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("ä¸€ã€æ€»ä½“ç»Ÿè®¡\n")
            f.write("-"*40 + "\n")
            f.write(f"æå–æ€»æ•°: {merge_stats['total']} æ¡\n")
            f.write(f"è¦†ç›–å…¬å¸: {merge_stats['companies']} å®¶\n\n")
            
            if 'Success Level' in df.columns:
                f.write("äºŒã€æˆåŠŸç‡åˆ†æ\n")
                f.write("-"*40 + "\n")
                success_counts = df['Success Level'].value_counts()
                for level, count in success_counts.items():
                    f.write(f"{level}: {count} ({count/len(df)*100:.1f}%)\n")
                f.write("\n")
            
            if 'Company' in df.columns:
                f.write("ä¸‰ã€TOP10å…¬å¸\n")
                f.write("-"*40 + "\n")
                top_companies = df['Company'].value_counts().head(10)
                for company, count in top_companies.items():
                    f.write(f"{company}: {count} æ¡\n")
            
            f.write("\n" + "="*80 + "\n")
            f.write("æŠ¥å‘Šç»“æŸ\n")
        
        print(f"\nğŸ“„ æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")


if __name__ == "__main__":
    # æµ‹è¯•åŠŸèƒ½
    show_status()